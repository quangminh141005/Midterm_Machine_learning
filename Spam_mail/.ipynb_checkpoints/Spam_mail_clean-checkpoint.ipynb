{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6b82b46d",
      "metadata": {},
      "source": [
        "# Spam Email Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bac0e00",
      "metadata": {},
      "source": [
        "## Imports\n",
        "All required libraries are imported here for clarity and maintainability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2e4b66f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, log_loss, roc_curve, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List\n",
        "import math\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bf69268",
      "metadata": {},
      "source": [
        "## Data Loading and Preprocessing\n",
        "Load the spam email dataset and preprocess it by mapping 'ham' to 0 and 'spam' to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c4fdda9d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Category                                            Message\n",
            "0      ham  Go until jurong point, crazy.. Available only ...\n",
            "1      ham                      Ok lar... Joking wif u oni...\n",
            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      ham  U dun say so early hor... U c already then say...\n",
            "4      ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Category                                            Message\n",
              "0         0  Go until jurong point, crazy.. Available only ...\n",
              "1         0                      Ok lar... Joking wif u oni...\n",
              "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3         0  U dun say so early hor... U c already then say...\n",
              "4         0  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('mail_data.csv')\n",
        "datalink = 'https://www.kaggle.com/datasets/shantanudhakadd/email-spam-detection-dataset-classification/data'\n",
        "print(df.head())\n",
        "\n",
        "# Map 'ham' to 0 and 'spam' to 1\n",
        "df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28cedaee",
      "metadata": {},
      "source": [
        "## TF-IDF Implementation\n",
        "Implement TF-IDF from scratch and using scikit-learn to convert email messages into numerical vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d5c3c1d",
      "metadata": {},
      "source": [
        "### Custom TF-IDF Implementation\n",
        "A custom function to compute TF-IDF vectors for the email messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "eca616cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_tfidf(documents):\n",
        "    \"\"\"Compute TF-IDF matrix for a list of documents.\n",
        "    \n",
        "    Args:\n",
        "        documents: List of text documents.\n",
        "    Returns:\n",
        "        numpy.ndarray: TF-IDF matrix where rows are documents and columns are words.\n",
        "    \"\"\"\n",
        "    # Tokenize and clean a document\n",
        "    def tokenize(doc):\n",
        "        doc = re.sub(r'[\\t\\n\\r]', ' ', doc)                 # Replace tabs/newlines\n",
        "        doc = re.sub(r'[^\\w\\s]', '', doc.lower())           # Remove punctuation and lowercase\n",
        "        doc = re.sub(r'\\s+', ' ', doc).strip()              # Normalize whitespace\n",
        "        return doc.split()                                  # Tokenize by splitting words\n",
        "\n",
        "    # Tokenize all documents\n",
        "    tokenized_docs = [tokenize(doc) for doc in documents]\n",
        "\n",
        "    # Build vocabulary\n",
        "    vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
        "\n",
        "    # Compute Term Frequency (TF)\n",
        "    def compute_tf(doc_tokens):\n",
        "        tf = {}\n",
        "        total_terms = len(doc_tokens)\n",
        "        if total_terms == 0:\n",
        "            return {word: 0 for word in vocab}\n",
        "        for word in vocab:\n",
        "            tf[word] = doc_tokens.count(word) / total_terms\n",
        "        return tf\n",
        "\n",
        "    tf_list = [compute_tf(doc) for doc in tokenized_docs]\n",
        "\n",
        "    # Compute Document Frequency (DF)\n",
        "    def compute_df(docs):\n",
        "        df = {}\n",
        "        for word in vocab:\n",
        "            df[word] = sum(1 for doc in docs if word in doc)\n",
        "        return df\n",
        "\n",
        "    df = compute_df(tokenized_docs)\n",
        "\n",
        "    # Compute Inverse Document Frequency (IDF)\n",
        "    def compute_idf(df, N):\n",
        "        idf = {}\n",
        "        for word, doc_count in df.items():\n",
        "            idf[word] = math.log(N / (1 + doc_count))\n",
        "        return idf\n",
        "\n",
        "    idf = compute_idf(df, len(documents))\n",
        "\n",
        "    # Compute TF-IDF for each document\n",
        "    def compute_tfidf_vector(tf, idf):\n",
        "        tfidf = {}\n",
        "        for word in vocab:\n",
        "            tfidf[word] = tf.get(word, 0) * idf.get(word, 0)\n",
        "        return tfidf\n",
        "\n",
        "    tfidf_list = [compute_tfidf_vector(tf, idf) for tf in tf_list]\n",
        "\n",
        "    # Round and clean up result\n",
        "    result = []\n",
        "    for tfidf in tfidf_list:\n",
        "        result.append({word: round(score, 4) for word, score in tfidf.items() if score > 0})\n",
        "\n",
        "    # Turn list of dicts into matrix\n",
        "    def tfidf_to_matrix(result, vocab):\n",
        "        vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "        matrix = np.zeros((len(result), len(vocab)))\n",
        "\n",
        "        for doc_idx, tfidf_dict in enumerate(result):\n",
        "            for word, tfidf in tfidf_dict.items():\n",
        "                if word in vocab_index:\n",
        "                    word_idx = vocab_index[word]\n",
        "                    matrix[doc_idx][word_idx] = tfidf\n",
        "\n",
        "        return matrix\n",
        "    \n",
        "    matrix = tfidf_to_matrix(result, vocab)\n",
        "\n",
        "    return matrix\n",
        "\n",
        "# Compute TF-IDF for the dataset\n",
        "dataset = compute_tfidf(df['Message'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "707ca038",
      "metadata": {},
      "source": [
        "### Scikit-learn TF-IDF Implementation\n",
        "Use scikit-learn's TfidfVectorizer for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "95e4b123",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess text: remove missing rows, convert to lowercase, remove punctuation\n",
        "scikitlearn_data = df['Message'].dropna().astype(str)\n",
        "scikitlearn_data = scikitlearn_data.str.lower().replace(r'[^\\w\\s]','',regex=True)\n",
        "\n",
        "# Create and apply TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(scikitlearn_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c37455",
      "metadata": {},
      "source": [
        "## Logistic Regression Implementation\n",
        "Implement logistic regression from scratch to classify emails as spam or ham."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ba707665",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"Apply sigmoid function to input.\n",
        "    \n",
        "    Args:\n",
        "        x: Input array.\n",
        "    Returns:\n",
        "        numpy.ndarray: Sigmoid of input.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def compute_loss(x_train, y_train, w, b):\n",
        "    \"\"\"Compute binary cross-entropy loss.\n",
        "    \n",
        "    Args:\n",
        "        x_train: Training features.\n",
        "        y_train: Training labels.\n",
        "        w: Weight vector.\n",
        "        b: Bias term.\n",
        "    Returns:\n",
        "        float: Loss value.\n",
        "    \"\"\"\n",
        "    m = x_train.shape[0]\n",
        "    z = np.dot(x_train, w) + b\n",
        "    h = sigmoid(z)\n",
        "    epsilon = 1e-15\n",
        "    loss = -np.mean(y_train * np.log(h + epsilon) + (1 - y_train) * np.log(1 - h + epsilon))\n",
        "    return loss\n",
        "\n",
        "def compute_gradient(x_train, y_train, w, b):\n",
        "    \"\"\"Compute gradients for weights and bias.\n",
        "    \n",
        "    Args:\n",
        "        x_train: Training features.\n",
        "        y_train: Training labels.\n",
        "        w: Weight vector.\n",
        "        b: Bias term.\n",
        "    Returns:\n",
        "        tuple: Gradients for weights (dw) and bias (db).\n",
        "    \"\"\"\n",
        "    m = x_train.shape[0]\n",
        "    z = np.dot(x_train, w) + b\n",
        "    h = sigmoid(z)\n",
        "    error = h - y_train\n",
        "    dw = np.dot(x_train.T, error) / m\n",
        "    db = np.sum(error) / m\n",
        "    return dw, db\n",
        "\n",
        "def gradient_descent(x_train, y_train, w, b, iters, lr):\n",
        "    \"\"\"Perform gradient descent to optimize logistic regression parameters.\\n\\nInformaci√≥n adicional: This function uses gradient descent to minimize the binary cross-entropy loss for a logistic regression model.\n",
        "    \n",
        "    Args:\n",
        "        x_train: Training features.\n",
        "        y_train: Training labels.\n",
        "        w: Initial weight vector.\n",
        "        b: Initial bias term.\n",
        "        iters: Number of iterations.\n",
        "        lr: Learning rate.\n",
        "    Returns:\n",
        "        tuple: Optimized weights, bias, and loss history.\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    for i in range(iters):\n",
        "        dw_dj, db_dj = compute_gradient(x_train, y_train, w, b)\n",
        "        w = w - lr * dw_dj\n",
        "        b = b - lr * db_dj\n",
        "        loss = compute_loss(x_train, y_train, w, b)\n",
        "        losses.append(loss)\n",
        "        if i % math.ceil(iters / 10) == 0:\n",
        "            print(f\"Iteration: {i}, Cost value: {compute_loss(x_train, y_train, w, b)}\")\n",
        "\n",
        "    print(\"Complete gradient descent!\")\n",
        "    print(f\"Weight: {w}, Bias: {b}\\n\")\n",
        "\n",
        "    return w, b, losses\n",
        "\n",
        "def check_threshold(x, threshold):\n",
        "    \"\"\"Apply threshold to classify predictions.\n",
        "    \n",
        "    Args:\n",
        "        x: Input probability.\n",
        "        threshold: Classification threshold.\n",
        "    Returns:\n",
        "        int: 0 if x < threshold, else 1.\n",
        "    \"\"\"\n",
        "    return 0 if x < threshold else 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a7bc6f5",
      "metadata": {},
      "source": [
        "## Data Splitting\n",
        "Split the dataset into training, validation, and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c6f7d541",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use custom TF-IDF features\n",
        "X = dataset\n",
        "y = np.array(df['Category'])\n",
        "\n",
        "# Split into train (70%), validation (15%), and test (15%)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f98b1f80",
      "metadata": {},
      "source": [
        "## Model Training\n",
        "Train logistic regression models with different learning rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "9b54c0c7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 0, Cost value: 0.6917698229501618\n",
            "Iteration: 1000, Cost value: 0.3973526189635055\n",
            "Iteration: 2000, Cost value: 0.38445490294125917\n",
            "Iteration: 3000, Cost value: 0.3794840859741482\n",
            "Iteration: 4000, Cost value: 0.3751333234050555\n",
            "Iteration: 5000, Cost value: 0.3708971221503404\n",
            "Iteration: 6000, Cost value: 0.36673145368222404\n",
            "Iteration: 7000, Cost value: 0.36263166572609196\n",
            "Iteration: 8000, Cost value: 0.3585966960138447\n",
            "Iteration: 9000, Cost value: 0.354625832969232\n",
            "Complete gradient descent!\n",
            "Weight: [ 0.01478585  0.00952728  0.0052442  ... -0.00446129 -0.15805026\n",
            " -0.00854035], Bias: -1.7624358685777506\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model 1: Learning rate = 0.01\n",
        "iters = 10000\n",
        "lr_1 = 0.01\n",
        "m, n = X_train.shape\n",
        "w_1 = np.zeros(n,)\n",
        "b_1 = 0\n",
        "w_1, b_1, loss_1 = gradient_descent(X_train, y_train, w_1, b_1, iters, lr_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b3e9d06c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration: 0, Cost value: 0.6862955321367522\n",
            "Iteration: 1000, Cost value: 0.3708798228917829\n",
            "Iteration: 2000, Cost value: 0.3507021697816155\n",
            "Iteration: 3000, Cost value: 0.3320952718367653\n",
            "Iteration: 4000, Cost value: 0.31497723712029835\n",
            "Iteration: 5000, Cost value: 0.29926147523819974\n",
            "Iteration: 6000, Cost value: 0.2848543210341894\n",
            "Iteration: 7000, Cost value: 0.2716572491979031\n",
            "Iteration: 8000, Cost value: 0.25957017455653236\n",
            "Iteration: 9000, Cost value: 0.2484945905929313\n",
            "Complete gradient descent!\n",
            "Weight: [ 0.06928406  0.04515783  0.0237665  ... -0.02193612 -0.56465445\n",
            " -0.03202776], Bias: -1.6705334268295895\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Model 2: Learning rate = 0.05\n",
        "lr_2 = 0.05\n",
        "m, n = X_train.shape\n",
        "w_2 = np.zeros(n,)\n",
        "b_2 = 0\n",
        "w_2, b_2, loss_2 = gradient_descent(X_train, y_train, w_2, b_2, iters, lr_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f5bab6",
      "metadata": {},
      "source": [
        "## Training Loss Visualization\n",
        "Plot the training loss curves for the models. Note: Plotting for Model 1 is included but incomplete in the original code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7585d033",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot loss for Model 1 (incomplete in original code)\n",
        "plt.plot(loss_1)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss value')\n",
        "plt.title('Training loss curve with lr = 0.01')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# TODO: Add plotting for Model 2 if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "346ff3e0",
      "metadata": {},
      "source": [
        "## Regularization\n",
        "Implement L2 regularization for logistic regression to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "802fcbd3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_loss_regularization(x_train, y_train, w, b, lambda_):\n",
        "    \"\"\"Compute binary cross-entropy loss with L2 regularization.\n",
        "    \n",
        "    Args:\n",
        "        x_train: Training features.\n",
        "        y_train: Training labels.\n",
        "        w: Weight vector.\n",
        "        b: Bias term.\n",
        "        lambda_: Regularization parameter.\n",
        "    Returns:\n",
        "        float: Loss value with L2 penalty.\n",
        "    \"\"\"\n",
        "    m = x_train.shape[0]\n",
        "    z = np.dot(x_train, w) + b\n",
        "    h = sigmoid(z)\n",
        "    epsilon = 1e-15\n",
        "    loss = -np.mean(y_train * np.log(h + epsilon) + (1 - y_train) * np.log(1 - h + epsilon))\n",
        "    l2_term = (lambda_ / (2 * m)) * np.sum(w**2)\n",
        "    return loss + l2_term\n",
        "\n",
        "def compute_gradient_regularization(x_train, y_train, w, b, lambda_):\n",
        "    \"\"\"Compute gradients for weights and bias with L2 regularization.\n",
        "    \n",
        "    Args:\n",
        "        x_train: Training features.\n",
        "        y_train: Training labels.\n",
        "        w: Weight vector.\n",
        "        b: Bias term.\n",
        "        lambda_: Regularization parameter.\n",
        "    Returns:\n",
        "        tuple: Gradients for weights (dw) and bias (db).\n",
        "    \"\"\"\n",
        "    m = x_train.shape[0]\n",
        "    z = np.dot(x_train, w) + b\n",
        "    h = sigmoid(z)\n",
        "    error = h - y_train\n",
        "    dw = (np.dot(x_train.T, error) + lambda_ * w) / m\n",
        "    db = np.sum(error) / m\n",
        "    return dw, db\n",
        "\n",
        "def gradient_descent_regularization(x_train, y_train, w, b, iters, lr, lambda_):\n",
        "    \"\"\"Perform gradient descent with L2 regularization.\n",
        "    \n",
        "    Args:\n",
        "        x_train: Training features.\n",
        "        y_train: Training labels.\n",
        "        w: Initial weight vector.\n",
        "        b: Initial bias term.\n",
        "        iters: Number of iterations.\n",
        "        lr: Learning rate.\n",
        "        lambda_: Regularization parameter.\n",
        "    Returns:\n",
        "        tuple: Optimized weights, bias, and loss history.\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    for i in range(iters):\n",
        "        dw_dj, db_dj = compute_gradient_regularization(x_train, y_train, w, b, lambda_)\n",
        "        w = w - lr * dw_dj\n",
        "        b = b - lr * db_dj\n",
        "        loss = compute_loss_regularization(x_train, y_train, w, b, lambda_)\n",
        "        losses.append(loss)\n",
        "        if i % math.ceil(iters / 2) == 0:\n",
        "            print(f\"Iteration: {i}, Cost value: {compute_loss(x_train, y_train, w, b)}\")\n",
        "\n",
        "    print(\"Complete gradient descent!\")\n",
        "    print(f\"Weight: {w}, Bias: {b}\\n\")\n",
        "\n",
        "    return w, b, losses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db9cecb5",
      "metadata": {},
      "source": [
        "## Model Evaluation with Regularization\n",
        "Train models with different regularization parameters and evaluate performance.\n",
        "\n",
        "**Note**: The original code included an evaluation loop with multiple `lambda_` values and thresholds, but it was incomplete (missing predictions). Below is a placeholder for the evaluation code, which needs to be completed with prediction generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "95fde306",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Placeholder for regularization evaluation\n",
        "# TODO: Complete this section by generating predictions and evaluating metrics\n",
        "iters = 10000\n",
        "lr = 0.1\n",
        "lambda_values = [0.001, 0.01, 0.1, 1, 10]\n",
        "thresholds = np.arange(0.1, 0.9, 0.05)\n",
        "\n",
        "# Example structure for evaluation (to be completed)\n",
        "for lambda_ in lambda_values:\n",
        "    m, n = X_train.shape\n",
        "    w_init = np.zeros(n)\n",
        "    b_init = 0\n",
        "    w, b, loss = gradient_descent_regularization(X_train, y_train, w_init, b_init, iters, lr, lambda_)\n",
        "    # Add prediction and evaluation code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61d6a791",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- The dataset (`mail_data.csv`) must be available in the working directory.\n",
        "- The evaluation section is incomplete and requires prediction generation and metric computation (e.g., accuracy, precision, recall, F1 score, ROC curve).\n",
        "- Consider adding scikit-learn's LogisticRegression for comparison with the custom implementation."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "env-jupyter",
      "language": "python",
      "name": "env_jupyter"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
