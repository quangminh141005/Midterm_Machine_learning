{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9963b271-4fcf-4144-8157-56456c165eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer# Import TF IDF \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from typing import List \n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a487817-c6d7-41c6-9bff-9df4280dbcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category                                            Message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('mail_data.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a6ff70-8a18-4e74-bf90-8580bc90111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3773df60-874d-4648-8e9f-a4cc0e2dcd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                            Message\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                      Ok lar... Joking wif u oni...\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say...\n",
       "4         0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57763cd1-c283-4e68-abe3-76d3428ccce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok lar... Joking wif u oni...\n"
     ]
    }
   ],
   "source": [
    "print(df['Message'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0fd35-e567-459d-9091-e7493c405de7",
   "metadata": {},
   "source": [
    "Implement `TF IDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "492af6aa-069e-45d3-a38d-a2c223cc87b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "def compute_tfidf(documents):\n",
    "    # 1. Tokenize and clean a document\n",
    "    def tokenize(doc):\n",
    "        doc = re.sub(r'[\\t\\n\\r]', ' ', doc)                 # Replace tabs/newlines\n",
    "        doc = re.sub(r'[^\\w\\s]', '', doc.lower())           # Remove punctuation and lowercase\n",
    "        doc = re.sub(r'\\s+', ' ', doc).strip()              # Normalize whitespace\n",
    "        return doc.split()                                  # Tokenize by splitting words\n",
    "\n",
    "    # 2. Tokenize all documents\n",
    "    tokenized_docs = [tokenize(doc) for doc in documents]\n",
    "\n",
    "    # 3. Build vocabulary\n",
    "    vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
    "\n",
    "    # 4. Compute Term Frequency (TF)\n",
    "    def compute_tf(doc_tokens):\n",
    "        tf = {}\n",
    "        total_terms = len(doc_tokens)\n",
    "        if total_terms == 0:\n",
    "            return {word: 0 for word in vocab}\n",
    "        for word in vocab:\n",
    "            tf[word] = doc_tokens.count(word) / total_terms\n",
    "        return tf\n",
    "\n",
    "    tf_list = [compute_tf(doc) for doc in tokenized_docs]\n",
    "\n",
    "    # 5. Compute Document Frequency (DF)\n",
    "    def compute_df(docs):\n",
    "        df = {}\n",
    "        for word in vocab:\n",
    "            df[word] = sum(1 for doc in docs if word in doc)\n",
    "        return df\n",
    "\n",
    "    df = compute_df(tokenized_docs)\n",
    "\n",
    "    # 6. Compute Inverse Document Frequency (IDF)\n",
    "    def compute_idf(df, N):\n",
    "        idf = {}\n",
    "        for word, doc_count in df.items():\n",
    "            idf[word] = math.log(N / (1 + doc_count))\n",
    "        return idf\n",
    "\n",
    "    idf = compute_idf(df, len(documents))\n",
    "\n",
    "    # 7. Compute TF-IDF for each document\n",
    "    def compute_tfidf_vector(tf, idf):\n",
    "        tfidf = {}\n",
    "        for word in vocab:\n",
    "            tfidf[word] = tf.get(word, 0) * idf.get(word, 0)\n",
    "        return tfidf\n",
    "\n",
    "    tfidf_list = [compute_tfidf_vector(tf, idf) for tf in tf_list]\n",
    "\n",
    "    # 8. Round and clean up result\n",
    "    result = []\n",
    "    for tfidf in tfidf_list:\n",
    "        result.append({word: round(score, 4) for word, score in tfidf.items() if score > 0})\n",
    "\n",
    "    # 9. Turn list of dicts into matrix\n",
    "    def tfidf_to_matrix(result, vocab):\n",
    "        vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "        matrix = np.zeros((len(result), len(vocab)))\n",
    "\n",
    "        for doc_idx, tfidf_dict in enumerate(result):\n",
    "            for word, tfidf in tfidf_dict.items():\n",
    "                if word in vocab_index:\n",
    "                    word_idx = vocab_index[word]\n",
    "                    matrix[doc_idx][word_idx] = tfidf\n",
    "\n",
    "        return matrix\n",
    "    \n",
    "    matrix =  tfidf_to_matrix(result, vocab)\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc0e1fc-f05e-476f-bbc4-cad1dad71fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = compute_tfidf(df['Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88186716-f24a-4ae2-9e23-e2c112bc4ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 9581)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4823a14-98f0-45ef-aa7d-da4ea6e6b7b4",
   "metadata": {},
   "source": [
    "# Implement TF-IDF using scikit-learn\n",
    "1. Using data(a list of sentence)\n",
    "2. Convert these text documents into numerical vectors\n",
    "    - Represent how important each word is in that document\n",
    "    - Based on its frequency(TF) and inverse document frequency(IDF)\n",
    "3. `fit()` the data\n",
    "    - Scans the text data and creates a dictionary of all word `TfidfVectorizer()`\n",
    "    - Calculate how rare or common each word is across all documents(IDF) `.fit(documents)`\n",
    "4. `transform()` the data(application step)\n",
    "    - Convert the input documents into a numerical matrix base on the vocabulary and  IDF that were learned during `fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ad908-6eca-4c91-94ad-66e6b8708401",
   "metadata": {},
   "outputs": [],
   "source": [
    "scikitlearn_data = df['Message'].dropna().astype(str) # Remove missing row\n",
    "# Create the vectorizer\n",
    "scikitlearn_data = scikitlearn_data.str.lower().replace(r'[^\\w\\s]','',regex=True)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform text\n",
    "tfidf_matrix = vectorizer.fit_transform(scikitlearn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316b43b3-94d4-4ed7-8de2-0b7b217ee92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(tfidf_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0026b-e085-466e-a28e-e34b0029fbf7",
   "metadata": {},
   "source": [
    "# Implement Logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9be30373-af7a-4243-a36f-9ac53268fed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1362e9a-7bab-45c0-bfd4-6cb1506f51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(x_train, y_train, w, b):\n",
    "    m = x_train.shape[0]\n",
    "    z = np.dot(x_train, w) + b\n",
    "    h = sigmoid(z)\n",
    "    epsilon = 1e-15\n",
    "    loss = -np.mean(y_train * np.log(h + epsilon) + (1 - y_train) *  np.log(1 - h + epsilon))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3afaa795-53b1-4466-bd0a-1ba568438a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(x_train, y_train, w, b):\n",
    "    m = x_train.shape[0]\n",
    "    z = np.dot(x_train, w) + b\n",
    "    h = sigmoid(z)\n",
    "    error = h - y_train\n",
    "    dw = np.dot(x_train.T, error) / m\n",
    "    db = np.sum(error) / m\n",
    "    return dw, db\n",
    "\n",
    "def gradient_descent(x_train, y_train, w, b, iters, lr):\n",
    "    losses = []\n",
    "    for i in range(iters):\n",
    "        dw_dj, db_dj = compute_gradient(x_train, y_train, w, b)\n",
    "        w = w - lr*dw_dj\n",
    "        b = b - lr*db_dj\n",
    "        loss = compute_loss(x_train, y_train, w, b)\n",
    "        losses.append(loss)\n",
    "        if i % math.ceil(iters / 10) == 0:\n",
    "            print(f\"Iteration: {i}, Cost value: {compute_loss(x_train, y_train, w, b)}\")\n",
    "\n",
    "    print(\"Complete gradient descent!\")\n",
    "    print(f\"Weight: {w}, Bias: {b}\\n\")\n",
    "\n",
    "    return w, b, losses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83c09293-25fa-434f-a983-feffe89e4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_threshold(x, threshold):\n",
    "    if x < threshold:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc43409e-e2e7-47e5-adda-75f52ea63f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset\n",
    "y = np.array(df['Category'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca22ff5a-e75f-4b17-ad4f-214fadf3bd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Cost value: 0.6917698229501618\n",
      "Iteration: 1000, Cost value: 0.3973526189635055\n",
      "Iteration: 2000, Cost value: 0.38445490294125917\n",
      "Iteration: 3000, Cost value: 0.3794840859741482\n",
      "Iteration: 4000, Cost value: 0.3751333234050555\n",
      "Iteration: 5000, Cost value: 0.3708971221503404\n",
      "Iteration: 6000, Cost value: 0.36673145368222404\n",
      "Iteration: 7000, Cost value: 0.36263166572609196\n",
      "Iteration: 8000, Cost value: 0.3585966960138447\n",
      "Iteration: 9000, Cost value: 0.354625832969232\n",
      "Complete gradient descent!\n",
      "Weight: [ 0.01478585  0.00952728  0.0052442  ... -0.00446129 -0.15805026\n",
      " -0.00854035], Bias: -1.7624358685777506\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iters = 10000\n",
    "lr = 0.01\n",
    "\n",
    "\n",
    "m,n = X_train.shape\n",
    "w = np.zeros(n,)\n",
    "b = 0\n",
    "\n",
    "\n",
    "w, b, loss = gradient_descent(X_train, y_train, w, b, iters, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a0f3b38-9e9b-4182-9c0a-ffae297125d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95695067264574\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn logistic regression\n",
    "X = tfidf_matrix\n",
    "y = np.array(df['Category'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1ecb5-fbae-42fa-b7ad-afee7eb1c8b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed854e-9cba-487d-9d09-cf94e2e1f07d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-jupyter",
   "language": "python",
   "name": "env_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
