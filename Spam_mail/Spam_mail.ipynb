{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9963b271-4fcf-4144-8157-56456c165eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # Import TF IDF \n",
    "from typing import List \n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a487817-c6d7-41c6-9bff-9df4280dbcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Category                                            Message\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('mail_data.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1a6ff70-8a18-4e74-bf90-8580bc90111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3773df60-874d-4648-8e9f-a4cc0e2dcd9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category                                            Message\n",
       "0         0  Go until jurong point, crazy.. Available only ...\n",
       "1         0                      Ok lar... Joking wif u oni...\n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3         0  U dun say so early hor... U c already then say...\n",
       "4         0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57763cd1-c283-4e68-abe3-76d3428ccce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok lar... Joking wif u oni...\n"
     ]
    }
   ],
   "source": [
    "print(df['Message'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0fd35-e567-459d-9091-e7493c405de7",
   "metadata": {},
   "source": [
    "Implement `TF IDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "492af6aa-069e-45d3-a38d-a2c223cc87b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "\n",
    "def compute_tfidf(documents):\n",
    "    # 1. Tokenize and clean a document\n",
    "    def tokenize(doc):\n",
    "        doc = re.sub(r'[\\t\\n\\r]', ' ', doc)                 # Replace tabs/newlines\n",
    "        doc = re.sub(r'[^\\w\\s]', '', doc.lower())           # Remove punctuation and lowercase\n",
    "        doc = re.sub(r'\\s+', ' ', doc).strip()              # Normalize whitespace\n",
    "        return doc.split()                                  # Tokenize by splitting words\n",
    "\n",
    "    # 2. Tokenize all documents\n",
    "    tokenized_docs = [tokenize(doc) for doc in documents]\n",
    "\n",
    "    # 3. Build vocabulary\n",
    "    vocab = sorted(set(word for doc in tokenized_docs for word in doc))\n",
    "\n",
    "    # 4. Compute Term Frequency (TF)\n",
    "    def compute_tf(doc_tokens):\n",
    "        tf = {}\n",
    "        total_terms = len(doc_tokens)\n",
    "        if total_terms == 0:\n",
    "            return {word: 0 for word in vocab}\n",
    "        for word in vocab:\n",
    "            tf[word] = doc_tokens.count(word) / total_terms\n",
    "        return tf\n",
    "\n",
    "    tf_list = [compute_tf(doc) for doc in tokenized_docs]\n",
    "\n",
    "    # 5. Compute Document Frequency (DF)\n",
    "    def compute_df(docs):\n",
    "        df = {}\n",
    "        for word in vocab:\n",
    "            df[word] = sum(1 for doc in docs if word in doc)\n",
    "        return df\n",
    "\n",
    "    df = compute_df(tokenized_docs)\n",
    "\n",
    "    # 6. Compute Inverse Document Frequency (IDF)\n",
    "    def compute_idf(df, N):\n",
    "        idf = {}\n",
    "        for word, doc_count in df.items():\n",
    "            idf[word] = math.log(N / (1 + doc_count))\n",
    "        return idf\n",
    "\n",
    "    idf = compute_idf(df, len(documents))\n",
    "\n",
    "    # 7. Compute TF-IDF for each document\n",
    "    def compute_tfidf_vector(tf, idf):\n",
    "        tfidf = {}\n",
    "        for word in vocab:\n",
    "            tfidf[word] = tf.get(word, 0) * idf.get(word, 0)\n",
    "        return tfidf\n",
    "\n",
    "    tfidf_list = [compute_tfidf_vector(tf, idf) for tf in tf_list]\n",
    "\n",
    "    # 8. Round and clean up result\n",
    "    result = []\n",
    "    for tfidf in tfidf_list:\n",
    "        result.append({word: round(score, 4) for word, score in tfidf.items() if score > 0})\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccc0e1fc-f05e-476f-bbc4-cad1dad71fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = compute_tfidf(df['Message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88186716-f24a-4ae2-9e23-e2c112bc4ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joking': 1.1133, 'lar': 0.8313, 'ok': 0.5033, 'oni': 1.1693, 'u': 0.3224, 'wif': 0.8883}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4823a14-98f0-45ef-aa7d-da4ea6e6b7b4",
   "metadata": {},
   "source": [
    "# Implement TF-IDF using scikit-learn\n",
    "1. Using data(a list of sentence)\n",
    "2. Convert these text documents into numerical vectors\n",
    "    - Represent how important each word is in that document\n",
    "    - Based on its frequency(TF) and inverse document frequency(IDF)\n",
    "3. `fit()` the data\n",
    "    - Scans the text data and creates a dictionary of all word `TfidfVectorizer()`\n",
    "    - Calculate how rare or common each word is across all documents(IDF) `.fit(documents)`\n",
    "4. `transform()` the data(application step)\n",
    "    - Convert the input documents into a numerical matrix base on the vocabulary and  IDF that were learned during `fit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "846ad908-6eca-4c91-94ad-66e6b8708401",
   "metadata": {},
   "outputs": [],
   "source": [
    "scikitlearn_data = df['Message'].dropna().astype(str) # Remove missing row\n",
    "# Create the vectorizer\n",
    "scikitlearn_data = scikitlearn_data.str.lower().replace(r'[^\\w\\s]','',regex=True)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform text\n",
    "tfidf_matrix = vectorizer.fit_transform(scikitlearn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "316b43b3-94d4-4ed7-8de2-0b7b217ee92e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'csr_matrix' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tfidf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n\u001b[1;32m----> 3\u001b[0m tfidf_matrix\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'csr_matrix' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "tfidf_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14e842-55f4-4505-aae9-da59ba5497ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-jupyter",
   "language": "python",
   "name": "env_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
